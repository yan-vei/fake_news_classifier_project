{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipo:\n",
    "Carvajal Hernandez, German Andres y\n",
    "Veitsman, Yana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import unicodedata\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>Algunas de las voces extremistas más conocida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>Después de casi dos años y medio de luchas po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>Dos periodistas birmanos de la agencia Reuter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>El Cuerpo Nacional de Policía ha detenido a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>El desfile de la firma en Roma se convierte e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>True</td>\n",
       "      <td>El Consejo de Gobierno ha dado su visto bueno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>True</td>\n",
       "      <td>Investigadores valencianos han desarrollado u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>True</td>\n",
       "      <td>Los arrestados actuaban en coches y en establ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>True</td>\n",
       "      <td>El Rey ha encargado este miércoles a Pedro Sá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>True</td>\n",
       "      <td>Las pruebas realizadas en el Centro Nacional ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               Text\n",
       "0      True   Algunas de las voces extremistas más conocida...\n",
       "1      True   Después de casi dos años y medio de luchas po...\n",
       "2      True   Dos periodistas birmanos de la agencia Reuter...\n",
       "3      True   El Cuerpo Nacional de Policía ha detenido a c...\n",
       "4      True   El desfile de la firma en Roma se convierte e...\n",
       "...     ...                                                ...\n",
       "1995   True   El Consejo de Gobierno ha dado su visto bueno...\n",
       "1996   True   Investigadores valencianos han desarrollado u...\n",
       "1997   True   Los arrestados actuaban en coches y en establ...\n",
       "1998   True   El Rey ha encargado este miércoles a Pedro Sá...\n",
       "1999   True   Las pruebas realizadas en el Centro Nacional ...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"fake_news.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a etiquetar todas las noticias no verdaderos con un \"1\" en nuestra lista de etiquetas. Esto se hace para que las medidas de performance del algoritmo estén relacionadas con la identifiación de noticias falsas.\n",
    "classes = data['class']\n",
    "y = []\n",
    "for i in classes:\n",
    "    if i == False:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a preparar diferentes funciones del preprocessado para compararlas después de elegir un modelo mejor\n",
    "\n",
    "# Esa función será una función básica para nuestra comparación\n",
    "def normalize_without_lemma(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [t for t in doc if t.text.isalpha() and t not in STOP_WORDS and len(t.text) > 2]\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        words.append(t.text.lower())\n",
    "    cleaned_text = \" \".join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "# Añadimos no palabras, pero lemas.\n",
    "def normalize_with_lemma(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [t.lemma_ for t in doc if t.text.isalpha() and t not in STOP_WORDS and len(t.text) > 2]\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        words.append(t.lower())\n",
    "    cleaned_text = \" \".join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "# Normalización sin extración de STOP_WORDs y palabras de longitud <= 2\n",
    "def basic_normalizing(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [t for t in doc if t.text.isalpha()]\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        words.append(t.text.lower())\n",
    "    cleaned_text = \" \".join(words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in texts:\n",
    "    X.append(normalize_without_lemma(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logitud de train set: 1400\n",
      "Logitud de test set: 600\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "print(\"Logitud de train set: \"+str(len(X_train)))\n",
    "print(\"Logitud de test set: \"+str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones BoW Train: (1400, 10330)\n",
      "Dimensiones BoW Test: (600, 10330)\n",
      "Dimensiones TF-IDF Train: (1400, 10330)\n",
      "Dimensiones TF-IDF Test: (600, 10330)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Extraemos las caracteristicas usando las tecnicas base de BoW y TF-IDF\n",
    "\n",
    "cv = CountVectorizer()\n",
    "train_bow = cv.fit_transform(X_train)\n",
    "test_bow = cv.transform(X_test)\n",
    "tv = TfidfVectorizer()\n",
    "train_tfidf=tv.fit_transform(X_train)\n",
    "test_tfidf=tv.transform(X_test)\n",
    "\n",
    "print(\"Dimensiones BoW Train: \"+str(train_bow.shape))\n",
    "print(\"Dimensiones BoW Test: \"+str(test_bow.shape))\n",
    "print(\"Dimensiones TF-IDF Train: \"+str(train_tfidf.shape))\n",
    "print(\"Dimensiones TF-IDF Test: \"+str(test_tfidf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones LSA BoW Train: (1400, 1000)\n",
      "Dimensiones LSA BoW Test: (600, 1000)\n",
      "Dimensiones LSA TF-IDF Train: (1400, 1000)\n",
      "Dimensiones LSA TF-IDF Test: (600, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Realizamos vectorizado con BoW y TF-IDF pero utilizando LSA para reducir dimensiones.\n",
    "\n",
    "cv = CountVectorizer()\n",
    "tv = TfidfVectorizer()\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "lsa_pipe_bow = make_pipeline(cv, svd)\n",
    "lsa_pipe_tfidf = make_pipeline(tv, svd)\n",
    "lsa_train_bow = lsa_pipe_bow.fit_transform(X_train)\n",
    "lsa_test_bow= lsa_pipe_bow.transform(X_test)\n",
    "lsa_train_tfidf = lsa_pipe_tfidf.fit_transform(X_train)\n",
    "lsa_test_tfidf= lsa_pipe_tfidf.transform(X_test)\n",
    "\n",
    "print(\"Dimensiones LSA BoW Train: \"+str(lsa_train_bow.shape))\n",
    "print(\"Dimensiones LSA BoW Test: \"+str(lsa_test_bow.shape))\n",
    "print(\"Dimensiones LSA TF-IDF Train: \"+str(lsa_train_tfidf.shape))\n",
    "print(\"Dimensiones LSA TF-IDF Test: \"+str(lsa_test_tfidf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:806: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(-1.0 * perword_bound)\n",
      "Dimensiones LDA BoW Train: (1400, 1000)\n",
      "Dimensiones LDA BoW Test: (600, 1000)\n",
      "Dimensiones LDA TF-IDF Train: (1400, 1000)\n",
      "Dimensiones LDA TF-IDF Test: (600, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#Realizamos vectorizado con BoW y TF-IDF pero utilizando LDA para reducir dimensiones.\n",
    "\n",
    "cv = CountVectorizer()\n",
    "tv = TfidfVectorizer()\n",
    "lda = LatentDirichletAllocation(n_components=1000, max_iter=5,\n",
    " learning_method='online',\n",
    " learning_offset=50.,\n",
    " random_state=0)\n",
    "lda_pipe_bow=make_pipeline(cv, lda)\n",
    "lda_pipe_tfidf=make_pipeline(tv, lda)\n",
    "lda_train_bow=lda_pipe_bow.fit_transform(X_train)\n",
    "lda_test_bow=lda_pipe_bow.transform(X_test)\n",
    "lda_train_tfidf=lda_pipe_tfidf.fit_transform(X_train)\n",
    "lda_test_tfidf=lda_pipe_tfidf.transform(X_test)\n",
    "\n",
    "print(\"Dimensiones LDA BoW Train: \"+str(lda_train_bow.shape))\n",
    "print(\"Dimensiones LDA BoW Test: \"+str(lda_test_bow.shape))\n",
    "print(\"Dimensiones LDA TF-IDF Train: \"+str(lda_train_tfidf.shape))\n",
    "print(\"Dimensiones LDA TF-IDF Test: \"+str(lda_test_tfidf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculamos distintas métricas sobre el\n",
    "    rendimiento del modelo. Devuelve un diccionario\n",
    "    con los parámetros medidos\"\"\"\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        3),\n",
    "        'Precision': np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'Recall': np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'F1 Score': np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3)}\n",
    "                        \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    \"\"\"Función que entrena un modelo de clasificación sobre\n",
    "    un conjunto de entrenamiento, lo aplica sobre un conjunto\n",
    "    de test y devuelve la predicción sobre el conjunto de test\n",
    "    y las métricas de rendimiento\"\"\"\n",
    "    # genera modelo    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predice usando el modelo sobre test\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evalúa rendimiento de la predicción   \n",
    "    metricas = get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions, metricas    \n",
    "\n",
    "modelLR = LogisticRegression(solver='liblinear')\n",
    "modelNB = GaussianNB()\n",
    "modelSVM = SGDClassifier(loss='hinge', max_iter=1000)\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "\n",
    "\n",
    "modelos = [('Logistic Regression', modelLR),\n",
    "           ('Naive Bayes', modelNB),\n",
    "           ('Linear SVM', modelSVM),\n",
    "            ('Gauss kernel SVM', modelRBFSVM)]\n",
    "\n",
    "metricas = []\n",
    "resultados = []\n",
    "\n",
    "# Modelos bow\n",
    "bow_train_features2 = train_bow.toarray()\n",
    "bow_test_features2 = test_bow.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} bow'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    " # Modelos tfidf\n",
    "tfidf_train_features2 = train_tfidf.toarray()\n",
    "tfidf_test_features2 = test_tfidf.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=tfidf_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LSA bow\n",
    "bow_train_features2 = lsa_train_bow\n",
    "bow_test_features2 = lsa_test_bow\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LSA bow'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LSA tfidf\n",
    "tfidf_train_features2 = lsa_train_tfidf\n",
    "tfidf_test_features2 = lsa_test_tfidf\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=tfidf_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LSA tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LDA bow\n",
    "bow_train_features2 = lda_train_bow\n",
    "bow_test_features2 = lda_test_bow\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LDA bow'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LDA tfidf\n",
    "tfidf_train_features2 = lda_train_tfidf\n",
    "tfidf_test_features2 = lda_test_tfidf\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=tfidf_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LDA tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy  Precision  Recall  F1 Score                         modelo\n",
      "7      0.782      0.782   0.782     0.782         Gauss kernel SVM tfidf\n",
      "15     0.772      0.773   0.772     0.771     Gauss kernel SVM LSA tfidf\n",
      "14     0.765      0.765   0.765     0.765           Linear SVM LSA tfidf\n",
      "6      0.763      0.763   0.763     0.763               Linear SVM tfidf\n",
      "12     0.757      0.757   0.757     0.757  Logistic Regression LSA tfidf\n",
      "4      0.752      0.752   0.752     0.752      Logistic Regression tfidf\n",
      "3      0.748      0.751   0.748     0.748           Gauss kernel SVM bow\n",
      "0      0.738      0.741   0.738     0.738        Logistic Regression bow\n",
      "8      0.737      0.738   0.737     0.737    Logistic Regression LSA bow\n",
      "1      0.737      0.746   0.737     0.736                Naive Bayes bow\n",
      "5      0.735      0.739   0.735     0.735              Naive Bayes tfidf\n",
      "11     0.733      0.734   0.733     0.733       Gauss kernel SVM LSA bow\n",
      "2      0.728      0.728   0.728     0.728                 Linear SVM bow\n",
      "10     0.707      0.719   0.707     0.704             Linear SVM LSA bow\n",
      "17     0.523      0.521   0.523     0.458            Naive Bayes LDA bow\n",
      "16     0.482      0.232   0.482     0.313    Logistic Regression LDA bow\n",
      "18     0.482      0.232   0.482     0.313             Linear SVM LDA bow\n",
      "19     0.482      0.232   0.482     0.313       Gauss kernel SVM LDA bow\n",
      "20     0.482      0.232   0.482     0.313  Logistic Regression LDA tfidf\n",
      "21     0.482      0.232   0.482     0.313          Naive Bayes LDA tfidf\n",
      "22     0.482      0.232   0.482     0.313           Linear SVM LDA tfidf\n",
      "23     0.482      0.232   0.482     0.313     Gauss kernel SVM LDA tfidf\n",
      "13     0.473      0.402   0.473     0.323          Naive Bayes LSA tfidf\n",
      "9      0.472      0.391   0.472     0.323            Naive Bayes LSA bow\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metricasDf=pd.DataFrame(metricas)\n",
    "\n",
    "\"\"\"\n",
    "Suponemos que la mejor métrica para evaluar nuestro modelo es el recall, porque el recall es en realidad el valor de \n",
    "los verdaderos positivos que nuestro modelo fue capaz de reconocer sobre todos los positivos. \n",
    "Dado que, en nuestro caso, la detección de un artículo falso se considera un caso positivo y se etiqueta como un 1, \n",
    "deberíamos basar nuestras conclusiones en el recuerdo\n",
    "\"\"\"\n",
    "\n",
    "metricasDf=metricasDf.sort_values(\"Recall\", ascending=False)\n",
    "print(metricasDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a elegir los dos mejores modelos y vamos a comparar si nuestra función del preprocessado cambiará algo\n",
    "\n",
    "# Primero, probamos el preprocessado con lematización\n",
    "X = []\n",
    "for i in texts:\n",
    "    X.append(normalize_with_lemma(i))\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(gamma='scale', C=2)\n",
    "\n",
    "train_tfidf=tv.fit_transform(X_train)\n",
    "test_tfidf=tv.transform(X_test)\n",
    "\n",
    "lsa_train_tfidf = lsa_pipe_tfidf.fit_transform(X_train)\n",
    "lsa_test_tfidf= lsa_pipe_tfidf.transform(X_test)\n",
    "\n",
    "prediccion_lematizado, metrica_lematizado = train_predict_evaluate_model(classifier=model,\n",
    "                                           train_features=train_tfidf,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=test_tfidf,\n",
    "                                           test_labels=y_test)\n",
    "\n",
    "prediccion_lematizado_lsa, metrica_lematizado_lsa = train_predict_evaluate_model(classifier=model,\n",
    "                                           train_features=lsa_train_tfidf,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=lsa_test_tfidf,\n",
    "                                           test_labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, probamos con el preprocessado muy básico\n",
    "\n",
    "X = []\n",
    "for i in texts:\n",
    "    X.append(basic_normalizing(i))\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf=tv.fit_transform(X_train)\n",
    "test_tfidf=tv.transform(X_test)\n",
    "\n",
    "lsa_train_tfidf = lsa_pipe_tfidf.fit_transform(X_train)\n",
    "lsa_test_tfidf= lsa_pipe_tfidf.transform(X_test)\n",
    "\n",
    "prediccion_basico, metrica_basico = train_predict_evaluate_model(classifier=model,\n",
    "                                           train_features=train_tfidf,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=test_tfidf,\n",
    "                                           test_labels=y_test)\n",
    "\n",
    "prediccion_basico_lsa, metrica_basico_lsa = train_predict_evaluate_model(classifier=model,\n",
    "                                           train_features=lsa_train_tfidf,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=lsa_test_tfidf,\n",
    "                                           test_labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Tipo de preprocessado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.782</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.782</td>\n",
       "      <td>Sin lematizacion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.782</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.782</td>\n",
       "      <td>Basico con stopwords y palabras cortas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.771</td>\n",
       "      <td>Sin lematizacion LSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.772</td>\n",
       "      <td>Con lematizacion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.763</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.763</td>\n",
       "      <td>Basico con stopwords y palabras cortas LSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.759</td>\n",
       "      <td>Con lematizacion LSA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score  \\\n",
       "0     0.782      0.782   0.782     0.782   \n",
       "4     0.782      0.782   0.782     0.782   \n",
       "1     0.772      0.773   0.772     0.771   \n",
       "2     0.772      0.774   0.772     0.772   \n",
       "5     0.763      0.764   0.763     0.763   \n",
       "3     0.760      0.770   0.760     0.759   \n",
       "\n",
       "                        Tipo de preprocessado  \n",
       "0                            Sin lematizacion  \n",
       "4      Basico con stopwords y palabras cortas  \n",
       "1                        Sin lematizacion LSA  \n",
       "2                            Con lematizacion  \n",
       "5  Basico con stopwords y palabras cortas LSA  \n",
       "3                        Con lematizacion LSA  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como podemos ver, hay pequeña diferencia entre diferentes maneras del preprocessado\n",
    "\n",
    "metrica_sin_lema = metricasDf[metricasDf['modelo']=='Gauss kernel SVM tfidf'].to_dict('records')[0]\n",
    "metrica_sin_lema_lsa = metricasDf[metricasDf['modelo']=='Gauss kernel SVM LSA tfidf'].to_dict('records')[0]\n",
    "preprocessados = ['Sin lematizacion', 'Sin lematizacion LSA','Con lematizacion', 'Con lematizacion LSA','Basico con stopwords y palabras cortas','Basico con stopwords y palabras cortas LSA']\n",
    "comparacion_prep = pd.DataFrame([metrica_sin_lema,metrica_sin_lema_lsa, metrica_lematizado,metrica_lematizado_lsa, metrica_basico,metrica_basico_lsa]).drop(['modelo'], axis=1)\n",
    "comparacion_prep['Tipo de preprocessado'] = preprocessados\n",
    "comparacion_prep.sort_values('Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Los resultados realmente no cambiaron. El modelo Gauss Kernel es el mejor en este caso, sin la reducción de dimensionalidad de LSA. Esto se debe posiblemente a que no hay suficientes documentos en el conjunto de datos para hacer una reducción significativa. Sin embargo, se observa que Ambos modelos prefieren un pre procesado sin lematización, ya que SVM es un modelo multidimensional que se beneficia de las diferencias entre los documentos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
