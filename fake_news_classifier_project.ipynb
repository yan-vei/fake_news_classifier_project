{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import unicodedata\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>Algunas de las voces extremistas más conocida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>Después de casi dos años y medio de luchas po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>Dos periodistas birmanos de la agencia Reuter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>El Cuerpo Nacional de Policía ha detenido a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>El desfile de la firma en Roma se convierte e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>True</td>\n",
       "      <td>El Consejo de Gobierno ha dado su visto bueno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>True</td>\n",
       "      <td>Investigadores valencianos han desarrollado u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>True</td>\n",
       "      <td>Los arrestados actuaban en coches y en establ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>True</td>\n",
       "      <td>El Rey ha encargado este miércoles a Pedro Sá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>True</td>\n",
       "      <td>Las pruebas realizadas en el Centro Nacional ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               Text\n",
       "0      True   Algunas de las voces extremistas más conocida...\n",
       "1      True   Después de casi dos años y medio de luchas po...\n",
       "2      True   Dos periodistas birmanos de la agencia Reuter...\n",
       "3      True   El Cuerpo Nacional de Policía ha detenido a c...\n",
       "4      True   El desfile de la firma en Roma se convierte e...\n",
       "...     ...                                                ...\n",
       "1995   True   El Consejo de Gobierno ha dado su visto bueno...\n",
       "1996   True   Investigadores valencianos han desarrollado u...\n",
       "1997   True   Los arrestados actuaban en coches y en establ...\n",
       "1998   True   El Rey ha encargado este miércoles a Pedro Sá...\n",
       "1999   True   Las pruebas realizadas en el Centro Nacional ...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"fake_news.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a etiquetar todas las noticias no verdaderos con un \"1\" en nuestra lista de etiquetas\n",
    "classes = data['class']\n",
    "y = []\n",
    "for i in classes:\n",
    "    if i == False:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a preparar diferentes funciones del preprocessado para compararlas después de elegir un modelo mejor\n",
    "\n",
    "# Esa función será una función básica para nuestra comparación\n",
    "def normalize_without_lemma(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [t for t in doc if t.text.isalpha() and t not in STOP_WORDS and len(t.text) > 2]\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        words.append(t.text.lower())\n",
    "    cleaned_text = \" \".join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "# Añadimos no palabras, pero lemas\n",
    "def normalize_with_lemma(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [t.lemma_ for t in doc if t.text.isalpha() and t not in STOP_WORDS and len(t.text) > 2]\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        words.append(t.lower())\n",
    "    cleaned_text = \" \".join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "# Normalización sin extración de STOP_WORDs y palabras de longitud <= 2\n",
    "def basic_normalizing(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [t for t in doc if t.text.isalpha()]\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        words.append(t.text.lower())\n",
    "    cleaned_text = \" \".join(words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in texts:\n",
    "    X.append(normalize_without_lemma(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "train_bow = cv.fit_transform(X_train)\n",
    "test_bow = cv.transform(X_test)\n",
    "tv = TfidfVectorizer()\n",
    "train_tfidf=tv.fit_transform(X_train)\n",
    "test_tfidf=tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "cv = CountVectorizer()\n",
    "tv = TfidfVectorizer()\n",
    "svd = TruncatedSVD(n_components=10)\n",
    "lsa_pipe_bow = make_pipeline(cv, svd)\n",
    "lsa_pipe_tfidf = make_pipeline(tv, svd)\n",
    "lsa_train_bow = lsa_pipe_bow.fit_transform(X_train)\n",
    "lsa_test_bow= lsa_pipe_bow.transform(X_test)\n",
    "lsa_train_tfidf = lsa_pipe_tfidf.fit_transform(X_train)\n",
    "lsa_test_tfidf= lsa_pipe_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "cv = CountVectorizer()\n",
    "tv = TfidfVectorizer()\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n",
    " learning_method='online',\n",
    " learning_offset=50.,\n",
    " random_state=0)\n",
    "lda_pipe_bow=make_pipeline(cv, lda)\n",
    "lda_pipe_tfidf=make_pipeline(tv, lda)\n",
    "lda_train_bow=lda_pipe_bow.fit_transform(X_train)\n",
    "lda_test_bow=lda_pipe_bow.transform(X_test)\n",
    "lda_train_tfidf=lda_pipe_tfidf.fit_transform(X_train)\n",
    "lda_test_tfidf=lda_pipe_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculamos distintas métricas sobre el\n",
    "    rendimiento del modelo. Devuelve un diccionario\n",
    "    con los parámetros medidos\"\"\"\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        3),\n",
    "        'Precision': np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'Recall': np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'F1 Score': np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3)}\n",
    "                        \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    \"\"\"Función que entrena un modelo de clasificación sobre\n",
    "    un conjunto de entrenamiento, lo aplica sobre un conjunto\n",
    "    de test y devuelve la predicción sobre el conjunto de test\n",
    "    y las métricas de rendimiento\"\"\"\n",
    "    # genera modelo    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predice usando el modelo sobre test\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evalúa rendimiento de la predicción   \n",
    "    metricas = get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions, metricas    \n",
    "\n",
    "modelLR = LogisticRegression(solver='liblinear')\n",
    "modelNB = GaussianNB()\n",
    "modelSVM = SGDClassifier(loss='hinge', max_iter=1000)\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "\n",
    "\n",
    "modelos = [('Logistic Regression', modelLR),\n",
    "           ('Naive Bayes', modelNB),\n",
    "           ('Linear SVM', modelSVM),\n",
    "            ('Gauss kernel SVM', modelRBFSVM)]\n",
    "\n",
    "metricas = []\n",
    "resultados = []\n",
    "\n",
    "# Modelos bow\n",
    "bow_train_features2 = train_bow.toarray()\n",
    "bow_test_features2 = test_bow.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} bow'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    " # Modelos tfidf\n",
    "tfidf_train_features2 = train_tfidf.toarray()\n",
    "tfidf_test_features2 = test_tfidf.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=tfidf_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LSA bow\n",
    "bow_train_features2 = lsa_train_bow\n",
    "bow_test_features2 = lsa_test_bow\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LSA bow'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LSA tfidf\n",
    "tfidf_train_features2 = lsa_train_tfidf\n",
    "tfidf_test_features2 = lsa_test_tfidf\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=tfidf_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LSA tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LDA bow\n",
    "bow_train_features2 = lda_train_bow\n",
    "bow_test_features2 = lda_test_bow\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LDA bow'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LDA tfidf\n",
    "tfidf_train_features2 = lda_train_tfidf\n",
    "tfidf_test_features2 = lda_test_tfidf\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=tfidf_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LDA tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy  Precision  Recall  F1 Score                         modelo\n",
      "7      0.773      0.776   0.773     0.773         Gauss kernel SVM tfidf\n",
      "6      0.758      0.762   0.758     0.758               Linear SVM tfidf\n",
      "3      0.748      0.751   0.748     0.748           Gauss kernel SVM bow\n",
      "4      0.747      0.750   0.747     0.747      Logistic Regression tfidf\n",
      "0      0.738      0.741   0.738     0.738        Logistic Regression bow\n",
      "1      0.737      0.746   0.737     0.736                Naive Bayes bow\n",
      "5      0.730      0.733   0.730     0.730              Naive Bayes tfidf\n",
      "2      0.705      0.708   0.705     0.705                 Linear SVM bow\n",
      "12     0.677      0.685   0.677     0.675  Logistic Regression LSA tfidf\n",
      "15     0.668      0.668   0.668     0.668     Gauss kernel SVM LSA tfidf\n",
      "14     0.660      0.663   0.660     0.656           Linear SVM LSA tfidf\n",
      "8      0.607      0.612   0.607     0.605    Logistic Regression LSA bow\n",
      "13     0.602      0.610   0.602     0.587          Naive Bayes LSA tfidf\n",
      "11     0.590      0.594   0.590     0.589       Gauss kernel SVM LSA bow\n",
      "9      0.575      0.599   0.575     0.558            Naive Bayes LSA bow\n",
      "10     0.567      0.601   0.567     0.540             Linear SVM LSA bow\n",
      "17     0.537      0.691   0.537     0.399            Naive Bayes LDA bow\n",
      "19     0.535      0.624   0.535     0.405       Gauss kernel SVM LDA bow\n",
      "18     0.528      0.693   0.528     0.379             Linear SVM LDA bow\n",
      "16     0.525      0.521   0.525     0.498    Logistic Regression LDA bow\n",
      "21     0.520      0.537   0.520     0.369          Naive Bayes LDA tfidf\n",
      "22     0.520      0.751   0.520     0.358           Linear SVM LDA tfidf\n",
      "23     0.488      0.501   0.488     0.430     Gauss kernel SVM LDA tfidf\n",
      "20     0.482      0.489   0.482     0.423  Logistic Regression LDA tfidf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metricasDf=pd.DataFrame(metricas)\n",
    "\n",
    "\"\"\"\n",
    "Suponemos que la mejor métrica para evaluar nuestro modelo es el recall, porque el recall es en realidad el valor de \n",
    "los verdaderos positivos que nuestro modelo fue capaz de reconocer sobre todos los positivos. \n",
    "Dado que, en nuestro caso, la detección de un artículo falso se considera un caso positivo y se etiqueta como un 1, \n",
    "deberíamos basar nuestras conclusiones en el recuerdo\n",
    "\"\"\"\n",
    "\n",
    "metricasDf=metricasDf.sort_values(\"Recall\", ascending=False)\n",
    "print(metricasDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a elegir el mejor modelo y vamos a comparar si nuestra función del preprocessado cambiará algo\n",
    "\n",
    "# Primero, probamos el preprocessado con lematización\n",
    "X = []\n",
    "for i in texts:\n",
    "    X.append(normalize_with_lemma(i))\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(gamma='scale', C=2)\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "train_tfidf=tv.fit_transform(X_train)\n",
    "test_tfidf=tv.transform(X_test)\n",
    "\n",
    "tfidf_train_features = train_tfidf.toarray()\n",
    "tfidf_test_features = test_tfidf.toarray()\n",
    "prediccion_lematizado, metrica_lematizado = train_predict_evaluate_model(classifier=model,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, probamos con el preprocessado muy básico\n",
    "\n",
    "X = []\n",
    "for i in texts:\n",
    "    X.append(basic_normalizing(i))\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer()\n",
    "train_tfidf=tv.fit_transform(X_train)\n",
    "test_tfidf=tv.transform(X_test)\n",
    "\n",
    "tfidf_train_features = train_tfidf.toarray()\n",
    "tfidf_test_features = test_tfidf.toarray()\n",
    "prediccion_basico, metrica_basico = train_predict_evaluate_model(classifier=model,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Tipo de preprocessado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.782</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.782</td>\n",
       "      <td>Basico con stopwords y palabras cortas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.773</td>\n",
       "      <td>Sin lematizacion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.772</td>\n",
       "      <td>Con lematizacion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score  \\\n",
       "2     0.782      0.782   0.782     0.782   \n",
       "0     0.773      0.776   0.773     0.773   \n",
       "1     0.772      0.774   0.772     0.772   \n",
       "\n",
       "                    Tipo de preprocessado  \n",
       "2  Basico con stopwords y palabras cortas  \n",
       "0                        Sin lematizacion  \n",
       "1                        Con lematizacion  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como podemos ver, hay pequeña diferencia entre diferentes maneras del preprocessado\n",
    "\n",
    "metrica_sin_lema = metricasDf[metricasDf['modelo']=='Gauss kernel SVM tfidf'].to_dict('records')[0]\n",
    "preprocessados = ['Sin lematizacion', 'Con lematizacion', 'Basico con stopwords y palabras cortas']\n",
    "comparacion_prep = pd.DataFrame([metrica_sin_lema, metrica_lematizado, metrica_basico]).drop(['modelo'], axis=1)\n",
    "comparacion_prep['Tipo de preprocessado'] = preprocessados\n",
    "comparacion_prep.sort_values('Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
