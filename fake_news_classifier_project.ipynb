{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import unicodedata\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      class                                               Text\n",
       "0      True   Algunas de las voces extremistas más conocida...\n",
       "1      True   Después de casi dos años y medio de luchas po...\n",
       "2      True   Dos periodistas birmanos de la agencia Reuter...\n",
       "3      True   El Cuerpo Nacional de Policía ha detenido a c...\n",
       "4      True   El desfile de la firma en Roma se convierte e...\n",
       "...     ...                                                ...\n",
       "1995   True   El Consejo de Gobierno ha dado su visto bueno...\n",
       "1996   True   Investigadores valencianos han desarrollado u...\n",
       "1997   True   Los arrestados actuaban en coches y en establ...\n",
       "1998   True   El Rey ha encargado este miércoles a Pedro Sá...\n",
       "1999   True   Las pruebas realizadas en el Centro Nacional ...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>True</td>\n      <td>Algunas de las voces extremistas más conocida...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>True</td>\n      <td>Después de casi dos años y medio de luchas po...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>True</td>\n      <td>Dos periodistas birmanos de la agencia Reuter...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>True</td>\n      <td>El Cuerpo Nacional de Policía ha detenido a c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>True</td>\n      <td>El desfile de la firma en Roma se convierte e...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>True</td>\n      <td>El Consejo de Gobierno ha dado su visto bueno...</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>True</td>\n      <td>Investigadores valencianos han desarrollado u...</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>True</td>\n      <td>Los arrestados actuaban en coches y en establ...</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>True</td>\n      <td>El Rey ha encargado este miércoles a Pedro Sá...</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>True</td>\n      <td>Las pruebas realizadas en el Centro Nacional ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data = pd.read_csv(\"fake_news.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data['class']\n",
    "y = []\n",
    "for i in classes:\n",
    "    if i == True:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [t for t in doc if t.text.isalpha() and t not in STOP_WORDS and len(t.text) > 2]\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        words.append(t.text.lower())\n",
    "    cleaned_text = \" \".join(words)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in texts:\n",
    "    X.append(normalize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "train_bow = cv.fit_transform(X_train)\n",
    "test_bow = cv.transform(X_test)\n",
    "tv = TfidfVectorizer()\n",
    "train_tfidf=tv.fit_transform(X_train)\n",
    "test_tfidf=tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "cv = CountVectorizer()\n",
    "tv = TfidfVectorizer()\n",
    "svd = TruncatedSVD(n_components=10)\n",
    "lsa_pipe_bow = make_pipeline(cv, svd)\n",
    "lsa_pipe_tfidf = make_pipeline(tv, svd)\n",
    "lsa_train_bow = lsa_pipe_bow.fit_transform(X_train)\n",
    "lsa_test_bow= lsa_pipe_bow.transform(X_test)\n",
    "lsa_train_tfidf = lsa_pipe_tfidf.fit_transform(X_train)\n",
    "lsa_test_tfidf= lsa_pipe_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "cv = CountVectorizer()\n",
    "tv = TfidfVectorizer()\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n",
    " learning_method='online',\n",
    " learning_offset=50.,\n",
    " random_state=0)\n",
    "lda_pipe_bow=make_pipeline(cv, lda)\n",
    "lda_pipe_tfidf=make_pipeline(tv, lda)\n",
    "lda_train_bow=lda_pipe_bow.fit_transform(X_train)\n",
    "lda_test_bow=lda_pipe_bow.transform(X_test)\n",
    "lda_train_tfidf=lda_pipe_tfidf.fit_transform(X_train)\n",
    "lda_test_tfidf=lda_pipe_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculamos distintas métricas sobre el\n",
    "    rendimiento del modelo. Devuelve un diccionario\n",
    "    con los parámetros medidos\"\"\"\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        3),\n",
    "        'Precision': np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'Recall': np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'F1 Score': np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3)}\n",
    "                        \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    \"\"\"Función que entrena un modelo de clasificación sobre\n",
    "    un conjunto de entrenamiento, lo aplica sobre un conjunto\n",
    "    de test y devuelve la predicción sobre el conjunto de test\n",
    "    y las métricas de rendimiento\"\"\"\n",
    "    # genera modelo    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predice usando el modelo sobre test\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evalúa rendimiento de la predicción   \n",
    "    metricas = get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions, metricas    \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelLR = LogisticRegression(solver='liblinear')\n",
    "modelNB = GaussianNB()\n",
    "modelSVM = SGDClassifier(loss='hinge', max_iter=1000)\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "\n",
    "\n",
    "modelos = [('Logistic Regression', modelLR),\n",
    "           ('Naive Bayes', modelNB),\n",
    "           ('Linear SVM', modelSVM),\n",
    "            ('Gauss kernel SVM', modelRBFSVM)]\n",
    "\n",
    "metricas = []\n",
    "resultados = []\n",
    "\n",
    "# Modelos bow\n",
    "bow_train_features2 = train_bow.toarray()\n",
    "bow_test_features2 = test_bow.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} bow'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    " # Modelos tfidf\n",
    "bow_train_features2 = train_tfidf.toarray()\n",
    "bow_test_features2 = test_tfidf.toarray()\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LSA bow\n",
    "bow_train_features2 = lsa_train_bow\n",
    "bow_test_features2 = lsa_test_bow\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LSA bow'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LSA tfidf\n",
    "bow_train_features2 = lsa_train_tfidf\n",
    "bow_test_features2 = lsa_test_tfidf\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LSA tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LDA bow\n",
    "bow_train_features2 = lda_train_bow\n",
    "bow_test_features2 = lda_test_bow\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LDA bow'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Modelos LDA bow\n",
    "bow_train_features2 = lda_train_tfidf\n",
    "bow_test_features2 = lda_test_tfidf\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=bow_train_features2,\n",
    "                                           train_labels=y_train,\n",
    "                                           test_features=bow_test_features2,\n",
    "                                           test_labels=y_test)\n",
    "    metrica['modelo']=f'{m} LDA tfidf'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    Accuracy  Precision  Recall  F1 Score                         modelo\n20     0.482      0.489   0.482     0.423  Logistic Regression LDA tfidf\n23     0.488      0.501   0.488     0.430     Gauss kernel SVM LDA tfidf\n22     0.495      0.513   0.495     0.437           Linear SVM LDA tfidf\n21     0.520      0.537   0.520     0.369          Naive Bayes LDA tfidf\n16     0.525      0.521   0.525     0.498    Logistic Regression LDA bow\n10     0.525      0.540   0.525     0.407             Linear SVM LSA bow\n18     0.527      0.684   0.527     0.375             Linear SVM LDA bow\n19     0.535      0.624   0.535     0.405       Gauss kernel SVM LDA bow\n17     0.537      0.691   0.537     0.399            Naive Bayes LDA bow\n13     0.557      0.562   0.557     0.530          Naive Bayes LSA tfidf\n9      0.575      0.600   0.575     0.558            Naive Bayes LSA bow\n11     0.590      0.594   0.590     0.589       Gauss kernel SVM LSA bow\n14     0.598      0.667   0.598     0.540           Linear SVM LSA tfidf\n8      0.612      0.617   0.612     0.610    Logistic Regression LSA bow\n12     0.648      0.654   0.648     0.647  Logistic Regression LSA tfidf\n15     0.650      0.651   0.650     0.647     Gauss kernel SVM LSA tfidf\n2      0.727      0.753   0.727     0.722                 Linear SVM bow\n5      0.730      0.733   0.730     0.730              Naive Bayes tfidf\n1      0.737      0.746   0.737     0.736                Naive Bayes bow\n0      0.738      0.741   0.738     0.738        Logistic Regression bow\n4      0.747      0.750   0.747     0.747      Logistic Regression tfidf\n3      0.748      0.751   0.748     0.748           Gauss kernel SVM bow\n6      0.765      0.767   0.765     0.765               Linear SVM tfidf\n7      0.773      0.776   0.773     0.773         Gauss kernel SVM tfidf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metricasDf=pd.DataFrame(metricas)\n",
    "# Ya que es un modelo con varias etiquetas tiene más sentido seleccionar el mejor por la cantidad de veces que acertó, por lo que se usa el accuracy\n",
    "metricasDf=metricasDf.sort_values(\"Accuracy\")\n",
    "print(metricasDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}